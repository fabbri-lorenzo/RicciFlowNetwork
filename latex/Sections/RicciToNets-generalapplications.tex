\section{ARI, Modularity, and Performance}
\label{sec:ricci_flow_networks}

In the study of complex networks, a fundamental task is to identify densely connected groups of nodes, commonly referred to as \emph{communities}. These communities often correspond to meaningful substructures such as friend groups in social networks, functionally related proteins in biological networks, or topics in citation networks. There are numerous algorithms that aim to extract communities---ranging from graph partitioning heuristics and centrality-based edge removal to statistical and probabilistic methods.

The Ricci Flow-based technique for network community detection focuses on the geometric viewpoint: an edge with significantly negative Ricci curvature may signify a \emph{bridge} between distinct communities, whereas edges with positive curvature are typically nestled within cohesive communities. Iteratively adjusting edge weights according to Ollivier Ricci curvature has the effect of “magnifying” bridging edges and “contracting” internal edges, ultimately making a subsequent threshold-based cut reveal the inherent clusters.

Here are the major steps (in broad terms) for using Ricci Flow to detect communities:
\begin{enumerate}
    \item \textbf{Initialization}: Assign an initial weight to each edge $(x,y)$. Often, one starts with uniform weights or with weights based on an existing property such as adjacency or similarity.
    \item \textbf{Probability Measures}: Choose how to define the measure $m_x$ at each node $x$. A popular simple choice is to put uniform weight on all neighbors of $x$, ensuring $\sum_{v \in \text{neighbors}(x)} m_x(v)=1$. Other weighting schemes (e.g., discounting more distant neighbors) can also be used.
    \item \textbf{Curvature Computation}: For each edge $(x,y)$, compute the Ollivier Ricci curvature $\kappa(x,y)$ by solving the discrete optimal transport problem and using 
    \[
    \kappa(x,y) = 1 - \frac{W(m_x, m_y)}{d(x,y)}.
    \]
    \item \textbf{Discrete Ricci Flow Update}: Adjust the edge weight according to
    \[
    w_{xy}^{(i+1)} = w_{xy}^{(i)} \;-\; \eta \,\kappa_{xy}^{(i)} \; d_{xy}^{(i)}.
    \]
    Often, one sets $\eta=1$ and updates all edges simultaneously, then recomputes shortest path distances $d(\cdot,\cdot)$ for the next iteration. The total number of iterations can be chosen based on convergence criteria or practical heuristics.
    \item \textbf{Network Surgery}: After a certain number of iterations, examine the distribution of edge weights. Typically, bridging edges (those connecting separate communities) will have grown in length. Choose a threshold $T$ such that edges with $w_{xy} > T$ are considered “cuts,” removing them from the graph. The connected components that remain are taken as the identified communities.
    \item \textbf{Post-Processing}: If a graph has hierarchical communities, additional steps (e.g., repeating the process within subcomponents) might be performed to further subdivide the clusters.
\end{enumerate}

This pipeline is quite flexible in terms of parameter choices (e.g., the measure definition, the number of flow iterations, the threshold for edge cuts) and can adapt to various network topologies. The next subsections explain how to measure the resulting partition quality, focusing on two widely employed tools: the \emph{Adjusted Rand Index (ARI)} and \emph{Modularity}.

\subsection{Adjusted Rand Index (ARI)}
\label{subsec:ARI_definition}

\subsubsection{General Definition}
The \emph{Adjusted Rand Index (ARI)} is a popular external validation measure to compare a discovered clustering with a known ground-truth partition. Suppose you have a set of $n$ items (in our case, the $n$ nodes of a network). Let $C = \{C_1, \ldots, C_r\}$ be a partition of these items into $r$ clusters found by some method, and let $G = \{G_1, \ldots, G_s\}$ be the ground-truth (or reference) partition into $s$ clusters. The Rand Index (RI) measures the fraction of item pairs that are \emph{consistently} assigned in both partitions (i.e., either assigned together in both or assigned to different clusters in both). 

Formally, the Rand Index is given by:
\[
\mathrm{RI}(C,G) \;=\; \frac{a + d}{a + b + c + d},
\]
where 
\begin{itemize}
    \item $a$ is the number of pairs of items that are in the same cluster in $C$ \emph{and} in the same cluster in $G$,
    \item $b$ is the number of pairs of items that are in the same cluster in $C$ but in different clusters in $G$,
    \item $c$ is the number of pairs that are in different clusters in $C$ but in the same cluster in $G$,
    \item $d$ is the number of pairs that are in different clusters in $C$ and in different clusters in $G$.
\end{itemize}
Because $a+d$ counts all the agreements (put together or kept apart) and $b+c$ counts the disagreements, $\mathrm{RI}(C,G)$ is between 0 and 1, with 1 meaning a perfect match of the partitions.

However, the Rand Index does not correct for chance agreement. The \emph{Adjusted Rand Index} refines this by subtracting the expected RI of random partitions and rescaling. One can define:
\[
\mathrm{ARI}(C,G) \;=\; \frac{\mathrm{RI}(C,G)\;-\;\mathrm{Expected}[\mathrm{RI}]}{\max(\mathrm{RI})\;-\;\mathrm{Expected}[\mathrm{RI}]},
\]
which yields a value that ranges from 0 (or negative, depending on definition) up to 1. Here, 1 indicates the clustering $C$ exactly matches the ground-truth $G$, whereas an ARI near 0 suggests random agreement.

\subsubsection{Applying ARI to Ricci Flow-based Community Detection}
When applying Ricci Flow on a network, one typically obtains a final partition of the graph by removing edges beyond a certain length. If a ground-truth labeling exists, we compute the ARI to assess how well these communities align with the reference. If $\mathrm{ARI}$ is high, that means the geometric approach successfully captured the intended grouping. By examining the ARI as a function of the threshold, one can also decide the best cutoff for the “network surgery.” In practice, one might do a range of threshold values, measure the ARI for each, and pick the threshold that maximizes it (assuming the ground-truth is known). In contexts where the ground-truth partition is not known, we might rely on internal validation measures, such as modularity, to guess a suitable threshold.

\subsection{Modularity}
\label{subsec:modularity_definition}

\subsubsection{Definition of Modularity}
\emph{Modularity} is one of the most commonly used internal metrics for community detection in networks. Proposed initially by Newman and Girvan, it quantifies how well a particular partition of the network divides the nodes into communities that are dense internally and sparse between each other.

Let us consider a network with $n$ nodes and $m$ edges (or total edge weight if it is a weighted graph). For a given partition of the network into $k$ communities, the modularity $Q$ is computed as:
\[
Q \;=\;\frac{1}{2m} \sum_{i,j}\Bigl(A_{ij} \;-\;\frac{d_i\,d_j}{2m}\Bigr) \,\delta(c_i, c_j),
\]
where $A_{ij}$ is the adjacency matrix (or the weight matrix), $d_i$ is the degree (or sum of weights) of node $i$, $c_i$ is the community label of node $i$, and $\delta(c_i, c_j)$ is 1 if $i$ and $j$ are in the same community and 0 otherwise. The term $\frac{d_i d_j}{2m}$ approximates the expected number of edges (or expected weight) between $i$ and $j$ if edges are distributed randomly but respect node degrees. High modularity indicates that the actual number of intra-community edges is significantly above random expectation.

\subsubsection{Modularity as a Stopping or Surgery Criterion}
When using Ricci Flow for community detection, one can track how modularity evolves as edges get re-weighted and as one tries different thresholds for cutting. Typically, there is an intuitive sweet spot where further cutting does not substantially improve the modularity and might begin to over-segment the network. 

A practical strategy is as follows:
\begin{enumerate}
    \item Perform a fixed number of Ricci Flow iterations. 
    \item For a range of potential cut thresholds $T_1, T_2, \dots, T_r$, remove edges with weight above $T_j$. 
    \item Compute modularity $Q_j$ for each $T_j$.
    \item Select the threshold $T_j$ that yields the maximum $Q_j$.
\end{enumerate}
This threshold selection process is akin to the notion of “neck pinches” or “surgeries” in the manifold setting: a large weight often signifies a bridging structure (negative curvature region grown large) that is “pinching off” from the main components. If an external ground-truth is known, one may prefer the threshold that simultaneously optimizes ARI and modularity. In the absence of external labels, maximizing modularity is a common choice to define the “best” partition.

\subsection{Interpreting Network ``Surgery'' in This Context}
\label{subsec:surgery}

A hallmark of the classical Ricci Flow with surgery on manifolds is that when the flow develops singularities (often visualized as “neck pinches”), the manifold is physically separated into topologically distinct pieces. Drawing an analogy, in discrete Ricci Flow on graphs, the edges that grow large (due to negative curvature) can be viewed as “singularities” or bridging regions, reminiscent of the neck that pinches in a continuous manifold. The act of removing these edges at some iteration is the direct analog of performing surgery on the manifold. After removing these “necks,” the graph breaks into connected components, each presumably representing a dense or well-curved subregion, i.e., a community.

In practice, we carry out this network surgery step once or multiple times, balancing the preservation of meaningful connectivity with the desire to isolate truly separated clusters. Because many real networks can exhibit hierarchical or multi-level community structures, it is possible that each sub-component can be further refined if we continue the process within it. This multi-scale approach can be repeated if one suspects nested communities.

\subsection{Algorithmic Complexity and Practical Considerations}
\label{subsec:complexity}

While the continuous Ricci Flow PDE can be computationally demanding in the smooth case, the discrete version has its own set of computational challenges. The major cost typically arises in:
\begin{enumerate}
    \item \textbf{Wasserstein Distances:} Computing $W(m_x, m_y)$ for each edge $(x,y)$. In a graph with $n$ nodes and $e$ edges, if we attempt an exact solution of the optimal transport problem, it can become computationally expensive. However, in many practical discrete settings, especially with local probability measures $m_x$ that concentrate mass on immediate neighbors, $W(m_x,m_y)$ can often be computed with simpler combinatorial formulas or approximations. 
    \item \textbf{Repeated Distance Computation:} After each iteration, we update edge weights and potentially must recompute shortest-path distances $d(x,y)$ for all node pairs or for edges. If $e$ is large, repeated all-pairs computations might be costly, unless efficient incremental shortest-path algorithms or approximate methods are used.
\end{enumerate}
Despite these costs, modern computational resources and heuristics usually make discrete Ricci Flow feasible for networks of moderate size. For very large networks (millions of edges), one might rely on approximations, sampling techniques, or simplified versions of curvature (e.g., proxies to Ollivier Ricci curvature).

In practice, one needs to choose three important parameters:
\begin{enumerate}
    \item \textbf{Number of Ricci Flow Iterations}: Stopping too soon might not emphasize bridging edges enough to isolate communities; iterating too long might lead to degeneracies (e.g., certain edges become extremely large or extremely small). An empirical strategy is to run a moderate number (e.g., 10--20 iterations) and check if measures like ARI (if a ground truth is available) or modularity saturate.
    \item \textbf{Cut Threshold for Network Surgery}: Typically chosen by scanning multiple thresholds and evaluating a measure of clustering quality (e.g., modularity) or ARI. 
    \item \textbf{Mass Distributions}: The simplest is uniform distribution on neighbors (sometimes with or without a fraction of mass at the node itself). More sophisticated distributions might downweight distant neighbors, especially in large or weighted networks.
\end{enumerate}

\subsection{Broader Theoretical Context and Future Directions}
From a more theoretical vantage point, \emph{why} does curvature---in the sense of Ollivier---detect communities so well? Intuitively, the geometry of a manifold with positive curvature is reminiscent of a cohesive, ``ball-like'' region, while negatively curved regions show hyperbolic expansions akin to branching. In network terms, cohesive subgraphs correspond to a “positive curvature signature” because local random walks or local mass distributions align more easily, whereas bridging edges or tree-like expansions create a negative curvature effect. 

Future directions could include:
\begin{itemize}
    \item Exploring \emph{Forman-Ricci} curvature or other discrete curvature notions to see how they compare in community detection or anomaly detection tasks.
    \item Combining \emph{higher-order} or \emph{simplicial} network topologies, in which edges, triangles, and higher-dimensional simplices each carry a curvature notion, potentially refining the detection of substructures.
    \item Investigating multi-scale or hierarchical surgery, systematically refining each sub-community using iterative curvature-based updates.
\end{itemize}

All these highlight that curvature-based community detection offers a distinctive geometric lens on networks, bridging rich ideas from differential geometry to the realm of data science and large-scale graph analysis.